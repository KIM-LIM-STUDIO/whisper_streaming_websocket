# -*- coding: utf-8 -*-
"""파인튜닝버전_virtual_views.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1soQEyxIbnKhwhGpAe3sQ6iAUPXvFX5kE
"""

import asyncio
import websockets
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from peft import PeftModel

# ====== 모델 로드 ======
BASE_MODEL = "mistralai/Mistral-7B-v0.1"  # HuggingFace에서 로그인 필요
LORA_MODEL_PATH = "./mistral-jaemini-lora"

print("🔧 모델 불러오는 중...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    device_map="auto",
    torch_dtype=torch.float16,
    # load_in_4bit=True,  # RAM 부족하면 사용
)

model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

print("✅ 모델 로딩 완료!")

# ====== WebSocket 서버 설정 ======
VIRTUAL_VIEWERS_HOST = "localhost"
VIRTUAL_VIEWERS_PORT = 8765
connected_clients = set()

async def generate_response(message: str):
    prompt = f"사용자: {message}\nAI:"
    output = pipe(prompt, max_new_tokens=100, do_sample=True, top_p=0.95, temperature=0.7)[0]['generated_text']
    return output.split("AI:")[-1].strip()

async def handle_client(websocket):
    print("🎉 클라이언트 접속!")
    connected_clients.add(websocket)
    try:
        async for message in websocket:
            print(f"👂 수신된 메시지: {message}")
            response = await generate_response(message)
            print(f"📤 응답: {response}")
            await websocket.send(json.dumps({"response": response}))
    except websockets.exceptions.ConnectionClosed:
        print("🔌 클라이언트 연결 종료")
    finally:
        connected_clients.remove(websocket)

async def main():
    async with websockets.serve(handle_client, VIRTUAL_VIEWERS_HOST, VIRTUAL_VIEWERS_PORT):
        print(f"🚀 WebSocket 서버 실행 중: ws://{VIRTUAL_VIEWERS_HOST}:{VIRTUAL_VIEWERS_PORT}")
        await asyncio.Future()  # 무한 대기

if __name__ == "__main__":
    asyncio.run(main())