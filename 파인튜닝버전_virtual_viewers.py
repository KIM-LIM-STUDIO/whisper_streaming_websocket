# -*- coding: utf-8 -*-
"""íŒŒì¸íŠœë‹ë²„ì „_virtual_views.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1soQEyxIbnKhwhGpAe3sQ6iAUPXvFX5kE
"""

import asyncio
import websockets
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from peft import PeftModel

# ====== ëª¨ë¸ ë¡œë“œ ======
BASE_MODEL = "mistralai/Mistral-7B-v0.1"  # HuggingFaceì—ì„œ ë¡œê·¸ì¸ í•„ìš”
LORA_MODEL_PATH = "./mistral-jaemini-lora"

print("ğŸ”§ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    device_map="auto",
    torch_dtype=torch.float16,
    # load_in_4bit=True,  # RAM ë¶€ì¡±í•˜ë©´ ì‚¬ìš©
)

model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

# ====== WebSocket ì„œë²„ ì„¤ì • ======
VIRTUAL_VIEWERS_HOST = "localhost"
VIRTUAL_VIEWERS_PORT = 8765
connected_clients = set()

async def generate_response(message: str):
    prompt = f"ì‚¬ìš©ì: {message}\nAI:"
    output = pipe(prompt, max_new_tokens=100, do_sample=True, top_p=0.95, temperature=0.7)[0]['generated_text']
    return output.split("AI:")[-1].strip()

async def handle_client(websocket):
    print("ğŸ‰ í´ë¼ì´ì–¸íŠ¸ ì ‘ì†!")
    connected_clients.add(websocket)
    try:
        async for message in websocket:
            print(f"ğŸ‘‚ ìˆ˜ì‹ ëœ ë©”ì‹œì§€: {message}")
            response = await generate_response(message)
            print(f"ğŸ“¤ ì‘ë‹µ: {response}")
            await websocket.send(json.dumps({"response": response}))
    except websockets.exceptions.ConnectionClosed:
        print("ğŸ”Œ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œ")
    finally:
        connected_clients.remove(websocket)

async def main():
    async with websockets.serve(handle_client, VIRTUAL_VIEWERS_HOST, VIRTUAL_VIEWERS_PORT):
        print(f"ğŸš€ WebSocket ì„œë²„ ì‹¤í–‰ ì¤‘: ws://{VIRTUAL_VIEWERS_HOST}:{VIRTUAL_VIEWERS_PORT}")
        await asyncio.Future()  # ë¬´í•œ ëŒ€ê¸°

if __name__ == "__main__":
    asyncio.run(main())